---
olaresManifest.version: '0.10.0'
olaresManifest.type: app

metadata:
  name: ollama
  icon: https://file.bttcdn.com/appstore/ollama/icon.png
  description: Get up and running with large language models.
  appid: ollama
  title: Ollama
  version: '1.1.11'
  categories:
    - Utilities_v112
    - Productivity

entrances:
  - name: terminal
    port: 80
    host: terminal
    title: Ollama
    icon: https://file.bttcdn.com/appstore/ollama/icon.png
    openMethod: default

  # - authLevel: private
  #   host: ollama
  #   icon: https://file.bttcdn.com/appstore/ollama/icon.png
  #   name: ollama
  #   openMethod: default
  #   port: 11434
  #   title: Ollama
  #   invisible: true

  - authLevel: internal
    host: ollamaclient
    icon: https://file.bttcdn.com/appstore/ollama/icon.png
    name: ollamaclient
    openMethod: default
    port: 8080
    title: Ollama API
    invisible: true

spec:
  versionName: '0.12.7'
  featuredImage: https://file.bttcdn.com/appstore/ollama/1.webp
  promoteImage:
    - https://file.bttcdn.com/appstore/ollama/1.webp
    - https://file.bttcdn.com/appstore/ollama/2.webp
    - https://file.bttcdn.com/appstore/ollama/3.webp

  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3	8B
    - Llama 3	70B
    - Phi 3 Mini	3.8B
    - Phi 3 Medium	14B
    - Gemma 2	9B
    - Gemma 2	27B
    - Mistral	7B
    - Moondream 2	1.4B
    - Neural Chat	7B
    - Starling	7B
    - Code Llama	7B
    - Llama 2 Uncensored	7B
    - LLaVA	7B
    - Solar	10.7B

  upgradeDescription: |
    Upgrade Ollama version to v0.12.7

    # New models
    - Qwen3-VL: Qwen3-VL is now available in all parameter sizes ranging from 2B to 235B
    - MiniMax-M2: a 230 Billion parameter model built for coding & agentic workflows available on Ollama's cloud

    # What's Changed
    - Model load failures now include more information on Windows
    - Fixed embedding results being incorrect when running embeddinggemma
    - Fixed gemma3n on Vulkan backend
    - Increased time allocated for ROCm to discover devices
    - Fixed truncation error when generating embeddings
    - Fixed request status code when running cloud models
    - The OpenAI-compatible /v1/embeddings endpoint now supports encoding_format parameter
    - Ollama will now parse tool calls that don't conform to {"name": name, "arguments": args} (thanks @rick-github!)
    - Fixed prompt processing reporting in the llama runner
    - Increase speed when scheduling models
    - Fixed issue where FROM <model> would not inherit RENDERER or PARSER commands

    Get full release note at: https://github.com/ollama/ollama/releases/tag/v0.12.7


  developer: ollama
  website: https://ollama.com/
  sourceCode: https://github.com/ollama/ollama
  submitter: Olares
  locale:
    - en-US
    - zh-CN
  doc: https://github.com/ollama/ollama/tree/main/docs
  license:
    - text: MIT
      url: https://github.com/ollama/ollama#MIT-1-ov-file

  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
  limitedCpu: 6200m
  requiredCpu: 150m
  requiredDisk: 50Mi
  limitedDisk: 500Gi
  limitedMemory: 40Gi
  requiredMemory: 4200Mi
  requiredGpu: 0
  limitedGpu: 16Gi
  {{- else }}
  requiredMemory: 64Mi
  limitedMemory: 800Mi
  requiredDisk: 50Mi
  limitedDisk: 200Mi
  requiredCpu: 10m
  limitedCpu: 800m
  {{- end }}

  supportArch:
    - amd64
    - arm64
  onlyAdmin: true

permission:
  appData: true
  appCache: true

options:
  apiTimeout: 0
  appScope:
  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
    clusterScoped: true
    appRef:
      - openwebui
      - perplexica
      - n8n
      - ragflow
      - lobechat
      - comfyuishare
      - ollama
  {{- else }}
    clusterScoped: false
  {{- end }}
  dependencies:
    - name: olares
      version: '>=1.12.1-0'
      type: system
  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
  {{- else }}
    - name: ollama
      type: application
      version: '>=1.1.3'
      mandatory: true
  {{- end }}

provider:
- name: ollamaclient
  entrance: ollamaclient
  paths: ["/*"]
  verbs: ["*"]
