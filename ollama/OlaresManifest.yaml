---
olaresManifest.version: '0.10.0'
olaresManifest.type: app

metadata:
  name: ollama
  icon: https://app.cdn.olares.com/appstore/ollama/icon.png
  description: Get up and running with large language models.
  appid: ollama
  title: Ollama
  version: '1.1.11'
  categories:
    - Utilities_v112
    - Productivity

entrances:
  - name: terminal
    port: 80
    host: terminal
    title: Ollama
    icon: https://app.cdn.olares.com/appstore/ollama/icon.png
    openMethod: default

  # - authLevel: private
  #   host: ollama
  #   icon: https://app.cdn.olares.com/appstore/ollama/icon.png
  #   name: ollama
  #   openMethod: default
  #   port: 11434
  #   title: Ollama
  #   invisible: true

  - authLevel: internal
    host: ollamaclient
    icon: https://app.cdn.olares.com/appstore/ollama/icon.png
    name: ollamaclient
    openMethod: default
    port: 8080
    title: Ollama API
    invisible: true

spec:
  versionName: '0.12.9'
  featuredImage: https://app.cdn.olares.com/appstore/ollama/1.webp
  promoteImage:
    - https://app.cdn.olares.com/appstore/ollama/1.webp
    - https://app.cdn.olares.com/appstore/ollama/2.webp
    - https://app.cdn.olares.com/appstore/ollama/3.webp

  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3	8B
    - Llama 3	70B
    - Phi 3 Mini	3.8B
    - Phi 3 Medium	14B
    - Gemma 2	9B
    - Gemma 2	27B
    - Mistral	7B
    - Moondream 2	1.4B
    - Neural Chat	7B
    - Starling	7B
    - Code Llama	7B
    - Llama 2 Uncensored	7B
    - LLaVA	7B
    - Solar	10.7B

  upgradeDescription: |
    Upgrade Ollama version to v0.12.9
    
    # New models
    - Qwen3-VL: Qwen3-VL is now available in all parameter sizes ranging from 2B to 235B
    - MiniMax-M2: a 230 Billion parameter model built for coding & agentic workflows available on Ollama's cloud
    
    # What's Changed
    - Fixed performance regression on CPU-only systems  
    - Improved qwen3-vl performance, now with flash attention enabled by default  
    - qwen3-vl now outputs less leading whitespace in responses when ¡°thinking¡±  
    - Fixed issue where deepseek-v3.1 thinking could not be disabled in Ollama¡¯s new app  
    - Fixed issue where qwen3-vl failed to interpret images with transparent backgrounds  
    - Ollama will now stop running a model before removing it via `ollama rm`  
    - Fixed issue where prompt processing was slower in Ollama¡¯s engine  
    - Unsupported iGPUs are now ignored during device discovery on Windows  

    **Full release notes:**  
    https://github.com/ollama/ollama/releases/tag/v0.12.9



  developer: ollama
  website: https://ollama.com/
  sourceCode: https://github.com/ollama/ollama
  submitter: Olares
  locale:
    - en-US
    - zh-CN
  doc: https://github.com/ollama/ollama/tree/main/docs
  license:
    - text: MIT
      url: https://github.com/ollama/ollama#MIT-1-ov-file

  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
  limitedCpu: 6200m
  requiredCpu: 150m
  requiredDisk: 50Mi
  limitedDisk: 500Gi
  limitedMemory: 40Gi
  requiredMemory: 4200Mi
  requiredGpu: 0
  limitedGpu: 16Gi
  {{- else }}
  requiredMemory: 64Mi
  limitedMemory: 800Mi
  requiredDisk: 50Mi
  limitedDisk: 200Mi
  requiredCpu: 10m
  limitedCpu: 800m
  {{- end }}

  supportArch:
    - amd64
    - arm64
  onlyAdmin: true

permission:
  appData: true
  appCache: true

options:
  apiTimeout: 0
  appScope:
  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
    clusterScoped: true
    appRef:
      - openwebui
      - perplexica
      - n8n
      - ragflow
      - lobechat
      - comfyuishare
      - ollama
  {{- else }}
    clusterScoped: false
  {{- end }}
  dependencies:
    - name: olares
      version: '>=1.12.1-0'
      type: system
  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
  {{- else }}
    - name: ollama
      type: application
      version: '>=1.1.3'
      mandatory: true
  {{- end }}

provider:
- name: ollamaclient
  entrance: ollamaclient
  paths: ["/*"]
  verbs: ["*"]
