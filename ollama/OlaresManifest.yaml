---
olaresManifest.version: "0.10.0"
olaresManifest.type: app

metadata:
  name: ollama
  icon: https://app.cdn.olares.com/appstore/ollama/icon.png
  description: Get up and running with large language models.
  appid: ollama
  title: Ollama
  version: "1.1.12"
  categories:
    - Utilities_v112
    - Productivity

entrances:
  - name: terminal
    port: 80
    host: terminal
    title: Ollama
    icon: https://app.cdn.olares.com/appstore/ollama/icon.png
    openMethod: default

  # - authLevel: private
  #   host: ollama
  #   icon: https://app.cdn.olares.com/appstore/ollama/icon.png
  #   name: ollama
  #   openMethod: default
  #   port: 11434
  #   title: Ollama
  #   invisible: true

  - authLevel: internal
    host: ollamaclient
    icon: https://app.cdn.olares.com/appstore/ollama/icon.png
    name: ollamaclient
    openMethod: default
    port: 8080
    title: Ollama API
    invisible: true

spec:
  versionName: "0.12.10"
  featuredImage: https://app.cdn.olares.com/appstore/ollama/1.webp
  promoteImage:
    - https://app.cdn.olares.com/appstore/ollama/1.webp
    - https://app.cdn.olares.com/appstore/ollama/2.webp
    - https://app.cdn.olares.com/appstore/ollama/3.webp

  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own models, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3 8B
    - Llama 3 70B
    - Phi 3 Mini 3.8B
    - Phi 3 Medium 14B
    - Gemma 2 9B
    - Gemma 2 27B
    - Mistral 7B
    - Moondream 2 1.4B
    - Neural Chat 7B
    - Starling 7B
    - Code Llama 7B
    - Llama 2 Uncensored 7B
    - LLaVA 7B
    - Solar 10.7B

  upgradeDescription: |
    Upgrade Ollama version to v0.12.10

    ollama run now works with embedding models
    ollama run can now run embedding models to generate vector embeddings from text:

      ollama run embeddinggemma "Hello world"

    Content can also be provided to ollama run via standard input:

      echo "Hello world" | ollama run embeddinggemma

    # What's Changed
    - Fixed errors when running qwen3-vl:235b and qwen3-vl:235b-instruct
    - Enable flash attention for Vulkan (currently needs to be built from source)
    - Add Vulkan memory detection for Intel GPU using DXGI+PDH
    - Ollama will now return tool call IDs from the /api/chat API
    - Fixed hanging due to CPU discovery
    - Ollama will now show login instructions when switching to a cloud model in interactive mode
    - Fix reading stale VRAM data
    - ollama run now works with embedding models

    **Full release notes:**
    https://github.com/ollama/ollama/releases/tag/v0.12.10

  developer: ollama
  website: https://ollama.com/
  sourceCode: https://github.com/ollama/ollama
  submitter: Olares
  locale:
    - en-US
    - zh-CN
  doc: https://github.com/ollama/ollama/tree/main/docs
  license:
    - text: MIT
      url: https://github.com/ollama/ollama#MIT-1-ov-file

  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
  limitedCpu: 6200m
  requiredCpu: 150m
  requiredDisk: 50Mi
  limitedDisk: 500Gi
  limitedMemory: 40Gi
  requiredMemory: 4200Mi
  requiredGpu: 0
  limitedGpu: 16Gi
  {{- else }}
  requiredMemory: 64Mi
  limitedMemory: 800Mi
  requiredDisk: 50Mi
  limitedDisk: 200Mi
  requiredCpu: 10m
  limitedCpu: 800m
  {{- end }}

  supportArch:
    - amd64
    - arm64
  onlyAdmin: true

permission:
  appData: true
  appCache: true

options:
  apiTimeout: 0
  appScope:
  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
    clusterScoped: true
    appRef:
      - openwebui
      - perplexica
      - n8n
      - ragflow
      - lobechat
      - comfyuishare
      - ollama
  {{- else }}
  clusterScoped: false
  {{- end }}
  dependencies:
    - name: olares
      version: '>=1.12.1-0'
      type: system
  {{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
  {{- else }}
    - name: ollama
      type: application
      version: '>=1.1.3'
      mandatory: true
  {{- end }}

provider:
  - name: ollamaclient
    entrance: ollamaclient
    paths:
      - "/*"
    verbs:
      - "*"
