metadata:
  title: Ollama
  description: Get up and running with large language models.

spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3 8B
    - Llama 3 70B
    - Phi 3 Mini 3.8B
    - Phi 3 Medium 14B
    - Gemma 2 9B
    - Gemma 2 27B
    - Mistral 7B
    - Moondream 2 1.4B
    - Neural Chat 7B
    - Starling 7B
    - Code Llama 7B
    - Llama 2 Uncensored 7B
    - LLaVA 7B
    - Solar 10.7B

  upgradeDescription: |
    Upgrade Ollama version to v0.12.10

    ollama run now works with embedding models
    ollama run can now run embedding models to generate vector embeddings from text:

      ollama run embeddinggemma "Hello world"

    Content can also be provided to ollama run via standard input:

      echo "Hello world" | ollama run embeddinggemma

    # What's Changed
    - Fixed errors when running qwen3-vl:235b and qwen3-vl:235b-instruct
    - Enable flash attention for Vulkan (currently needs to be built from source)
    - Add Vulkan memory detection for Intel GPU using DXGI+PDH
    - Ollama will now return tool call IDs from the /api/chat API
    - Fixed hanging due to CPU discovery
    - Ollama will now show login instructions when switching to a cloud model in interactive mode
    - Fix reading stale VRAM data
    - ollama run now works with embedding models

    **Full release notes:**
    https://github.com/ollama/ollama/releases/tag/v0.12.10
