metadata:
  title: Ollama
  description: Get up and running with large language models.

spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3 8B
    - Llama 3 70B
    - Phi 3 Mini 3.8B
    - Phi 3 Medium 14B
    - Gemma 2 9B
    - Gemma 2 27B
    - Mistral 7B
    - Moondream 2 1.4B
    - Neural Chat 7B
    - Starling 7B
    - Code Llama 7B
    - Llama 2 Uncensored 7B
    - LLaVA 7B
    - Solar 10.7B

  upgradeDescription: |
    Upgrade Ollama version to v0.12.5

    # New models
    - Qwen3 Embedding: state of the art open embedding model by the Qwen team
    - DeepSeek-V3.1-Terminus: DeepSeek-V3.1-Terminus is a hybrid model that supports both thinking mode and non-thinking mode. It delivers more stable & reliable outputs across benchmarks compared to the previous version:

    # What's Changed
    - Thinking models now support structured outputs when using the /api/chat API
    - Ollama's app will now wait until Ollama is running to allow for a conversation to be started
    - Fixed issue where "think": false would show an error instead of being silently ignored
    - Fixed deepseek-r1 output issues
    - macOS 12 Monterey and macOS 13 Ventura are no longer supported
    - AMD gfx900 and gfx906 (MI50, MI60, etc) GPUs are no longer supported via ROCm. We're working to support these GPUs via Vulkan in a future release.

    Get full release note at: https://github.com/ollama/ollama/releases/tag/v0.12.5
