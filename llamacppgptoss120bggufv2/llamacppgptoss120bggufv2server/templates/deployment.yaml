{{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
{{- $llamaDomainENV := split "," .Values.domain.llamaclient -}}
{{- $llamaDomain := index $llamaDomainENV "_0" -}}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama
  namespace: {{ .Release.Namespace }}
  labels:
    io.kompose.service: llama
  annotations:
    applications.app.bytetrade.io/gpu-inject: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: llama
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        io.kompose.network/chrome-default: "true"
        io.kompose.service: llama
    spec:
      restartPolicy: Always
      containers:
        - name: llama
          image: "docker.io/beclab/ggml-org-llama.cpp:server-cuda-b7224"
          command:
            - /bin/sh
            - -c
            - |
              DONE_FILE="/models/.gpt-oss-120b-GGUF.done"
              echo "[wait] Waiting for $DONE_FILE ..."
              until [ -f "$DONE_FILE" ]; do
                sleep 2
              done
              echo "[wait] Detected $DONE_FILE, starting server."
              exec ./llama-server "$@"
          args:
            - "--server"
            - "-m"
            - "/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8080"
            - "-c"
            - "8000"
            - "--flash-attn"
            - "auto"
            - "--jinja"
            - "--reasoning-format"
            - "none"
            - "--n-gpu-layers"
            - "60"
            - "--n-cpu-moe"
            - "24"
            - "--temp"
            - "1.0"
            - "--top-p"
            - "1.0"
            - "--top-k"
            - "0"
            - "--threads"
            - "20"
          env:
            - name: PGID
              value: "1000"
            - name: PUID
              value: "1000"
            - name: TZ
              value: "Etc/UTC"
            - name: LLAMA_CACHE
              value: "/models/cache"
            - name: HF_HOME
              value: "/models/.cache/huggingface"
            - name: HUGGINGFACE_HUB_CACHE
              value: "/models/hub"
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              cpu: "19"
              memory: 69Gi
            requests:
              cpu: "6"
              memory: 20Gi
          volumeMounts:
            - name: data
              mountPath: /models
        - name: download-model
          image: "docker.io/beclab/harveyff-hf-downloader:v0.1.0"
          env:
            - name: HF_ENDPOINT
              value: "{{ .Values.olaresEnv.OLARES_USER_HUGGINGFACE_SERVICE }}"
            - name: HF_TOKEN
              value: "{{ .Values.olaresEnv.OLARES_USER_HUGGINGFACE_TOKEN }}"
            - name: VLLM_PROXY_TARGET
              value: http://localhost:8080
            - name: MODEL_NAME
              value: gpt-oss-120b
            - name: APP_URL
              value: "https://{{ $llamaDomain }}"
          args:
            - "--repo"
            - "ggml-org/gpt-oss-120b-GGUF"
            - "--ref"
            - "main"
            - "--outdir"
            - "/data"
            - "--static"
            - "/app/static"
            - "--port"
            - "8090"
            - "--probe-url"
            - "/health"
            - "--done-name"
            - ".gpt-oss-120b-GGUF.done"
          ports:
            - name: http
              containerPort: 8090
          startupProbe:
            httpGet:
              path: /ping
              port: 8090
            initialDelaySeconds: 10
            timeoutSeconds: 10
            periodSeconds: 10
            failureThreshold: 10
          livenessProbe:
            httpGet:
              path: /ping
              port: 8090
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 60
            periodSeconds: 60
            successThreshold: 1
            failureThreshold: 10
          volumeMounts:
            - name: data
              mountPath: /data
          resources:
            requests:
              cpu: 100m
              memory: 500Mi
            limits:
              cpu: "1"
              memory: 1Gi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      volumes:
        - name: data
          hostPath:
            path: "{{ .Values.userspace.userData }}/Huggingface/gptoss120bgguf"
            type: DirectoryOrCreate
---
apiVersion: v1
kind: Service
metadata:
  name: download-svc
  namespace: {{ .Release.Namespace }}
  labels:
    io.kompose.service: llama
spec:
  selector:
    io.kompose.service: llama
  ports:
    - name: http
      port: 8090
      targetPort: 8090
  type: ClusterIP
  
---
apiVersion: v1
kind: Service
metadata:
  name: sharedentrances-api
  namespace: {{ .Release.Namespace }}
  labels:
    io.kompose.service: llama
spec:
  selector:
    io.kompose.service: llama
  ports:
    - name: http
      port: 80
      targetPort: 8090
  type: ClusterIP
  
{{- end }}
