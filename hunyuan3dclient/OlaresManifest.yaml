olaresManifest.version: '0.10.0'
olaresManifest.type: app
metadata:
  name: hunyuan3dclient
  description: A Unified Framework for Text-to-3D and Image-to-3D Generation
  icon: https://file.bttcdn.com/appstore/hunyuan3d/logo.png
  appid: hunyuan3dclient
  version: '1.0.1'
  title: Hunyuan3D
  categories:
    - Creativity
    - Utilities
entrances:
- name: hunyuan3dclient
  host: hunyuan3dclient
  port: 8080
  title: Hunyuan3D
  icon: https://file.bttcdn.com/appstore/hunyuan3d/logo.png
  authLevel: private
  openMethod: window
permission:
  appData: true
  appCache: true
spec:
  promoteImage:
  - https://file.bttcdn.com/appstore/hunyuan3d/teaser.jpg
  versionName: '1.0.0'
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is the corresponding client app of "Hunyuan3D For Cluster". It requires "Hunyuan3D For Cluster" to be installed on your Olares cluster first.

    ## OVERVIEW ##
    Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation

    Features: 
    - First open-source model that supports text-and image-conditioned 3d generation both
    - With strong generalization ability and controllability, our model can reconstruct objects of various scales, from large structures like buildings to small items like tools and plants.
    - End-to-end generation within 10 seconds,including mesh and texture extraction.
    
    Abstract: 
    While 3D generative models have greatly improved artists' workflows, the existing diffusion models for 3D generation suffer from slow generation and poor generalization. To address this issue, we propose a two-stage approach named Hunyuan3D-1.0 including a lite version and a standard version, that both support text- and image-conditioned generation.

    In the first stage, we employ a multi-view diffusion model that efficiently generates multi-view RGB in approximately 4 seconds. These multi-view images capture rich details of the 3D asset from different viewpoints, relaxing the tasks from single-view to multi-view reconstruction. In the second stage, we introduce a feed-forward reconstruction model that rapidly and faithfully reconstructs the 3D asset given the generated multi-view images in approximately 7 seconds. The reconstruction network learns to handle noises and in-consistency introduced by the multi-view diffusion and leverages the available information from the condition image to efficiently recover the 3D structure.

    Our framework involves the text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to support both text- and image-conditioned 3D generation. Our standard version has 3x more parameters than our lite and other existing model. Our Hunyuan3D-1.0 achieves an impressive balance between speed and quality, significantly reducing generation time while maintaining the quality and diversity of the produced assets.
    
  developer: Tencent
  website: https://3d.hunyuan.tencent.com/
  sourceCode: https://github.com/Tencent/Hunyuan3D-1
  submitter: Olares
  locale:
  - en-US
  - zh-CN
  doc: https://github.com/Tencent/Hunyuan3D-1?tab=readme-ov-file
  requiredMemory: 32Mi
  limitedMemory: 256Mi
  requiredDisk: 56Mi
  limitedDisk: 1Gi
  requiredCpu: 10m
  limitedCpu: 100m
  supportArch:
  - amd64
options:
  dependencies:
  - name: olares
    type: system
    version: '>=1.10.1-0'
  - name: hunyuan3d
    type: application
    version: '>=1.0.0'
    mandatory: true