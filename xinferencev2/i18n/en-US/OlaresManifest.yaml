metadata:
  title: Xinference
  description: Model Serving Made Easy
spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Xorbits Inference(Xinference) is a powerful and versatile library designed to serve language, speech recognition, and multimodal models. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command. Whether you are a researcher, developer, or data scientist, Xorbits Inference empowers you to unleash the full potential of cutting-edge AI models.

    Key Features
    Model Serving Made Easy: Simplify the process of serving large language, speech recognition, and multimodal models. You can set up and deploy your models for experimentation and production with a single command.

    State-of-the-Art Models: Experiment with cutting-edge built-in models using a single command. Inference provides access to state-of-the-art open-source models!

    Heterogeneous Hardware Utilization: Make the most of your hardware resources with ggml. Xorbits Inference intelligently utilizes heterogeneous hardware, including GPUs and CPUs, to accelerate your model inference tasks.

    Flexible API and Interfaces: Offer multiple interfaces for interacting with your models, supporting OpenAI compatible RESTful API (including Function Calling API), RPC, CLI and WebUI for seamless model management and interaction.

    Distributed Deployment: Excel in distributed deployment scenarios, allowing the seamless distribution of model inference across multiple devices or machines.

    Built-in Integration with Third-Party Libraries: Xorbits Inference seamlessly integrates with popular third-party libraries including LangChain, LlamaIndex, Dify, and Chatbox.

  upgradeDescription: |
    ## Xinference v2.0.0 Release Notes
    
    ### New Features
    - Added support for multiple new models:
      * Qwen3-VL-Embedding-2B/8B (Embedding models)
      * Qwen3-VL-Reranker-2B/8B (Reranker models)
      * MinerU2.5-2509-1.2B (OCR model)
      * GLM-4.6 (Large Language Model)
      * Z-Image (Image model)
    - Added video GGUF cache management
    - Support for chat_template.jinja templates
    - UI improvements: Use browser locale as default language, add official website and model hub links
    
    ### Performance Optimizations
    - Increased Nginx proxy timeout configuration to resolve 504 timeout issues when launching models
      * proxy_send_timeout: 60s → 3600s
      * proxy_read_timeout: 1800s → 3600s
      * proxy_connect_timeout: 60s → 300s
    - Optimized LLM cache configuration to skip unnecessary downloads
    
    ### Bug Fixes
    - Fixed transformers version compatibility issue (< 5.0.0)
    - Fixed cache manager blocking issue during downloads
    - Fixed multiple known issues
    
    ### References
    - Release Notes: https://github.com/xorbitsai/inference/releases/tag/v2.0.0    