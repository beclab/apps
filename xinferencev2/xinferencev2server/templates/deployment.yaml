{{- if and .Values.admin .Values.bfl.username (eq .Values.admin .Values.bfl.username) }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    io.kompose.service: xinference
  name: xinference
  namespace: '{{ .Release.Namespace }}'
  annotations:
    applications.app.bytetrade.io/gpu-inject: "true"  
spec:
  replicas: 1
  selector:
    matchLabels:
      io.kompose.service: xinference
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        io.kompose.network/chrome-default: "true"
        io.kompose.service: xinference
    spec:
      # initContainers:
      # - name: init-chmod-data
      #   image: 'docker.io/beclab/aboveos-busybox:1.37.0'
      #   command:
      #     - sh
      #     - '-c'
      #     - |
      #       chown -R 1000:1000 /app-data-root
      #   resources: {}
      #   volumeMounts:
      #     - name:  data
      #       mountPath: /app-data-root
      #   terminationMessagePath: /dev/termination-log
      #   terminationMessagePolicy: File
      #   imagePullPolicy: IfNotPresent
      #   securityContext:
      #     runAsUser: 0
      containers:
      - name: xinference
        image: "beclab/xprobe-xinference:v2.0.0"
        command: ["sh", "-c", "unset LD_PRELOAD;xinference-local -H 0.0.0.0 --log-level debug"]
        args:
          - xinference-local
          - '-H'
          - 0.0.0.0
          - '--log-level'
          - debug
        ports:
          - containerPort: 9997
            protocol: TCP
          - containerPort: 8000
            protocol: TCP
        env:
          - name: XINFERENCE_MODEL_SRC
            value: modelscope
          - name: VLLM_PORT
            value: "8000"
          - name: HF_ENDPOINT
            value: "{{ .Values.olaresEnv.OLARES_USER_HUGGINGFACE_SERVICE }}"
        resources:
          limits:
            cpu: "5"
            memory: 20Gi
          requests:
            cpu: "1"
            memory: 2Gi
        volumeMounts:
        - mountPath: /root
          name: data
        - name: shm
          mountPath: /dev/shm
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        imagePullPolicy: IfNotPresent
      # - env:
      #   - name: PGID
      #     value: "1000"
      #   - name: PUID
      #     value: "1000"
      #   - name: TZ
      #     value: Etc/UTC
      #   - name: CUDA_INFO_FROM_OS
      #     value: "{{ .Values.GPU.Cuda }}"
      #   - name: GPU_TYPE_FROM_OS
      #     value: "{{ .Values.GPU.Type }}"
      #   - name: HF_ENDPOINT
      #     value: 'https://hf-mirror.com/'
      #   - name: VLLM_PORT
      #     value: "8000"
      #   image: vllm/vllm-openai:latest
      #   securityContext:
      #     runAsUser: 0
      #   command: ["sh", "-c", "unset LD_PRELOAD;python3 -m vllm.entrypoints.openai.api_server --trust-remote-code --chat-template=/vllm-workspace/examples/template_chatml.jinja;"]
      #   name: vllm
      #   ports:
      #   - containerPort: 8000
      #   resources:
      #     limits:
      #       cpu: "5"
      #       memory: 20Gi
      #     requests:
      #       cpu: "1"
      #       memory: 2Gi
      #   volumeMounts:
      #   - mountPath: /root/.cache/huggingface
      #     name: data
      #   - name: shm
      #     mountPath: /dev/shm
      restartPolicy: Always
      volumes:
      - hostPath:
          path: '{{ .Values.userspace.appCache }}'
          type: DirectoryOrCreate
        name: data
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: "2Gi"
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    io.kompose.service: xinference
  name: xinference
  namespace: '{{ .Release.Namespace }}'
spec:
  ports:
  - name: "8000"
    port: 8000
    targetPort: 8000
  - name: "9997"
    port: 9997
    targetPort: 9997
  selector:
    io.kompose.service: xinference
status:
  loadBalancer: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    io.kompose.service: xinference
  name: sharedentrances-api
  namespace: '{{ .Release.Namespace }}'
spec:
  ports:
  - name: "sharedentrances-api"
    port: 80
    targetPort: 9997
  selector:
    io.kompose.service: xinference
status:
  loadBalancer: {}
{{- end }}