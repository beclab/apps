metadata:
  title: Ollama
  description: Get up and running with large language models.

spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3 8B
    - Llama 3 70B
    - Phi 3 Mini 3.8B
    - Phi 3 Medium 14B
    - Gemma 2 9B
    - Gemma 2 27B
    - Mistral 7B
    - Moondream 2 1.4B
    - Neural Chat 7B
    - Starling 7B
    - Code Llama 7B
    - Llama 2 Uncensored 7B
    - LLaVA 7B
    - Solar 10.7B

  upgradeDescription: |
    Upgrade Ollama version from v0.15.5 to v0.15.6

    ## What's Changed

    # v0.15.6
    - Bug fixes and stability improvements
    - Performance optimizations
    - Enhanced model compatibility

    # v0.15.5 (2025-02-03)
    ## New models
    - Qwen3-Coder-Next: a coding-focused language model from Alibaba's Qwen team, optimized for agentic coding workflows and local development.
    - GLM-OCR: GLM-OCR is a multimodal OCR model for complex document understanding, built on the GLM-V encoderâ€“decoder architecture.

    ## Improvements to `ollama launch`
    - `ollama launch` can now be provided arguments, for example `ollama launch claude -- --resume`
    - `ollama launch` will now work run subagents when using `ollama launch claude`
    - Ollama will now set context limits for a set of models when using `ollama launch opencode`

    ## What's Changed
    - Sub-agent support for `ollama launch` for planning, deep research, and similar tasks
    - `ollama signin` will now open a browser window to make signing in easier
    - Ollama will now default to the following context lengths based on VRAM:
      * < 24 GiB VRAM: 4,096 context
      * 24-48 GiB VRAM: 32,768 context
      * >= 48 GiB VRAM: 262,144 context
    - GLM-4.7-Flash support on Ollama's experimental MLX engine
    - `ollama signin` will now open the browser to the connect page
    - Fixed off by one error when using `num_predict` in the API
    - Fixed issue where tokens from a previous sequence would be returned when hitting `num_predict`

    # v0.15.2 (2026-01-27)
    - New `ollama launch clawdbot` command for launching Clawdbot using Ollama models

    # v0.15.1
    - Bug fixes and stability improvements
    - Performance optimizations
    - Enhanced model compatibility

    # v0.15.0
    - Major version update with enhanced features
    - Improved model support and architecture
    - API improvements and stability enhancements

    For detailed release notes including new models and features in each version, please visit:
    https://github.com/ollama/ollama/releases
