metadata:
  title: Ollama
  description: 开始使用并运行大型语言模型吧。

spec:
  fullDescription: |
    ## 使用须知 ##
    该应用为共享应用，Olares 管理员安装后，集群中的所有用户都可以通过授权应用程序来调用它。

    ## 应用概览 ##
    Ollama 是一个用于在本地运行大型语言模型（LLMs）的用户友好界面。  
    对研究人员、开发者以及任何想要尝试大语言模型的人来说，Ollama 是一个非常有价值的工具。  
    使用 Ollama，您可以轻松下载和运行 LLMs，自定义并创建您自己的模型，并使用您设备上的文件与 LLMs 进行对话。

    Ollama 支持广泛的模型，包括：
    - Llama 3 8B
    - Llama 3 70B
    - Phi 3 Mini 3.8B
    - Phi 3 Medium 14B
    - Gemma 2 9B
    - Gemma 2 27B
    - Mistral 7B
    - Moondream 2 1.4B
    - Neural Chat 7B
    - Starling 7B
    - Code Llama 7B
    - Llama 2 Uncensored 7B
    - LLaVA 7B
    - Solar 10.7B

  upgradeDescription: |
    将 Ollama 版本从 v0.15.5 升级至 v0.15.6

    ## 主要变更

    # v0.15.6
    - 错误修复和稳定性改进
    - 性能优化
    - 增强模型兼容性

    # v0.15.5 (2025-02-03)
    ## 新增模型
    - Qwen3-Coder-Next：来自阿里巴巴 Qwen 团队的代码专用语言模型，针对智能体编码工作流和本地开发进行了优化。
    - GLM-OCR：GLM-OCR 是一个用于复杂文档理解的多模态 OCR 模型，基于 GLM-V 编码器-解码器架构构建。

    ## `ollama launch` 功能改进
    - `ollama launch` 现在可以接受参数，例如 `ollama launch claude -- --resume`
    - `ollama launch` 现在在使用 `ollama launch claude` 时可以运行子代理
    - 当使用 `ollama launch opencode` 时，Ollama 现在会为一系列模型设置上下文限制

    ## 主要变更
    - 子代理支持：`ollama launch` 现在支持子代理，用于规划、深度研究等类似任务
    - 改进的登录体验：`ollama signin` 现在会打开浏览器窗口，使登录更加便捷
    - 默认上下文长度：Ollama 现在根据 VRAM 自动设置以下默认上下文长度：
      * < 24 GiB VRAM: 4,096 上下文
      * 24-48 GiB VRAM: 32,768 上下文
      * >= 48 GiB VRAM: 262,144 上下文
    - MLX 引擎支持：GLM-4.7-Flash 现在支持 Ollama 的实验性 MLX 引擎
    - 浏览器连接：`ollama signin` 现在会打开浏览器到连接页面
    - Bug 修复：
      * 修复了在 API 中使用 `num_predict` 时的差一错误
      * 修复了在达到 `num_predict` 时返回前一个序列的 token 的问题

    # v0.15.2 (2026-01-27)
    - 新功能：新增 `ollama launch clawdbot` 命令，用于启动使用 Ollama 模型的 Clawdbot

    # v0.15.1
    - 错误修复和稳定性改进
    - 性能优化
    - 增强模型兼容性

    # v0.15.0
    - 主要版本更新，包含增强功能
    - 改进模型支持和架构
    - API 改进和稳定性增强

    如需查看每个版本的新增模型和功能的详细发布说明，请访问：
    https://github.com/ollama/ollama/releases
