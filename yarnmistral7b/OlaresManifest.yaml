olaresManifest.version: '0.8.1'
olaresManifest.type: model
metadata:
  name: yarnmistral7b
  description: A state-of-the-art language model for long context.
  icon: https://file.bttcdn.com/appstore/default/defaulticon.webp
  appid: yarnmistral7b
  version: '1.0.0'
  title: Yarn Mistral 7B Q4
  categories:
  - Finetuned
spec:
  versionName: '1.0'
  featuredImage: https://file.bttcdn.com/appstore/yarnmistral7b/featuredImage.png
  fullDescription: |
    Nous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method.

    It is an extension of Mistral-7B-v0.1 and supports a 128k token context window.

    Model Name: Yarn Mistral 7B 128K
    Model Creator: NousResearch
    Model Type: mistral
    Quantized by: TheBloke
    Library Name: transformers

    Base model: NousResearch/Yarn-Mistral-7b-128k

    Datasets:
    - emozilla/yarn-train-tokenized-16k-mistral

  upgradeDescription: |
    Update featuredImage
  developer: NousResearch, The Bloke
  website: https://huggingface.co/TheBloke/Yarn-Mistral-7B-128k-GGUF/
  submitter: Olares
  doc: https://huggingface.co/TheBloke/Yarn-Mistral-7B-128k-GGUF/
  locale:
  - en
  modelSize: 7B
  license:
  - text: Apache-2.0
    url: https://huggingface.co/TheBloke/Yarn-Mistral-7B-128k-GGUF/
  requiredMemory: 32Mi
  limitedMemory: 64Mi
  requiredDisk: 4.1Gi
  limitedDisk: 4.1Gi
  requiredGpu: 4Gi
  limitedGpu: 8Gi
  requiredCpu: 0.25
  limitedCpu: 0.5
  supportArch:
  - amd64
  - arm64
options:
  dependencies:
  - name: olares
    type: system
    version: '>=1.10.1-0'
  appScope:
    clusterScoped: true
